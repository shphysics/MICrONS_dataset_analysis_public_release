{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3da00bf",
   "metadata": {},
   "source": [
    "sources are based on: https://github.com/AllenInstitute/MicronsBinder/tree/master/notebooks/mm3_intro "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e1dab",
   "metadata": {},
   "source": [
    "# setting up workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298b2f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from caveclient import CAVEclient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial as sci_spatial\n",
    "from scipy.spatial import distance_matrix\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pickle #how to use pickle: https://www.datacamp.com/tutorial/pickle-python-tutorial \n",
    "import utils\n",
    "from nglui import statebuilder\n",
    "import plotly.figure_factory as ff\n",
    "import networkx as nx\n",
    "from itertools import chain, combinations\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import random\n",
    "import sklearn\n",
    "from collections import defaultdict\n",
    "\n",
    "client = CAVEclient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9eb0f8",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment the following line below to get new token if one has not previously done so; comment out if one has already done \n",
    "# client.auth.get_new_token()\n",
    "\n",
    "# uncomment the following line below to get new token if one has not previously done so; comment out if one has already done \n",
    "# client.auth.save_token(token=\"55d33f46f502c5c22535abf93c68cdb0\")\n",
    "\n",
    "# double checking the token number \n",
    "# auth = client.auth\n",
    "# print(f\"My current token is: {auth.token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161dd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load up the dataset through query # no query for minnie35: https://github.com/seung-lab/CAVEclient/issues/49 \n",
    "client = CAVEclient('minnie65_public_v117') #minnie65_public_v117\n",
    "# client2 = CAVEclient('minnie35_public_v0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0835b4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nucleus_detection_v0',\n",
       " 'synapses_pni_2',\n",
       " 'nucleus_neuron_svm',\n",
       " 'proofreading_status_public_release',\n",
       " 'func_unit_em_match_release',\n",
       " 'allen_soma_ei_class_model_v1',\n",
       " 'allen_visp_column_soma_coarse_types_v1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the tables we can query from the materialization engine\n",
    "client.materialize.get_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb747cf",
   "metadata": {},
   "source": [
    "working to connect the pre-post synaptic graph. source: https://github.com/AllenInstitute/MicronsBinder/blob/master/notebooks/mm3_intro/SynapseAndAnnotationQuery.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c67048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows you the basic information about this datastack within CAVE\n",
    "# client.info.get_datastack_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c5a329",
   "metadata": {},
   "source": [
    "1. Get the \"exteneded neurons\" from 'proofreading_status_public_release' (refer to the resource code for loading the file. (also add lines to double check and make sure that pt-root-id indeed equals to the valid-id) \n",
    "\n",
    "2.1 After getting all the \"extended neurons\" IDs we then go back to the method of using these IDs to find the synaptic IDs for each extended neuron ID and set up the dictionary. The dictionary will have the folloiwng structure: Key: synaptic ID (id) Vals: a tuple (pre-syn-neuron id aka pre_pt_root_id, post-syn-neuron id aka post_pt_root_id)\n",
    "\n",
    "2.2 transofrm the voxel coordinates into the xyz coordinates by element-wise-multiplying voxel pos and (4, 4, 40) to get um in x-y-z coordinates \n",
    "\n",
    "3. feed the coordinates info into the kd-tree using fcn from scipy. (source: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) \n",
    "4. For each synapse, query the kd-tree with a 10 um ball centered on that synapse, and for any neighboring synapse it finds, check to see if they share the same post segmentation ID.\n",
    "5. (current state) pick a given point, get its closest (10 um) neighbors and then view those points in the Micronexploer/neuroglancer for visual confirmation/verification.\n",
    "\n",
    "** will be quite interesting to see number of clusters on a given dendrites.\n",
    "\n",
    "** And we are also interested in finding the number above that is required to achieve making the spike sequences meaningful when relaying the informaiton. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e124e2",
   "metadata": {},
   "source": [
    "1. Get the \"exteneded neurons\" from 'proofreading_status_public_release' (refer to the resource code for loading the file. (also add lines to double check and make sure that pt-root-id indeed equals to the valid-id)\n",
    "\n",
    "** definition of \"extended\": 'extended' indicates that the cell is both clean and all tips have been followed as far as a proofreader was able to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f026614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the proofreaded neurons (with different proofreadbility)\n",
    "prf_df=client.materialize.query_table('nucleus_detection_v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e18be7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abe4d5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>valid</th>\n",
       "      <th>volume</th>\n",
       "      <th>pt_supervoxel_id</th>\n",
       "      <th>pt_root_id</th>\n",
       "      <th>pt_position</th>\n",
       "      <th>bb_start_position</th>\n",
       "      <th>bb_end_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>730537</td>\n",
       "      <td>t</td>\n",
       "      <td>32.307937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[381312, 273984, 19993]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373879</td>\n",
       "      <td>t</td>\n",
       "      <td>229.045043</td>\n",
       "      <td>96218056992431305</td>\n",
       "      <td>864691135538371698</td>\n",
       "      <td>[228816, 239776, 19593]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>601340</td>\n",
       "      <td>t</td>\n",
       "      <td>426.138010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[340000, 279152, 20946]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201858</td>\n",
       "      <td>t</td>\n",
       "      <td>93.753836</td>\n",
       "      <td>84955554103121097</td>\n",
       "      <td>864691135373893678</td>\n",
       "      <td>[146848, 213600, 26267]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600774</td>\n",
       "      <td>t</td>\n",
       "      <td>135.189791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[339120, 276112, 19442]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id valid      volume   pt_supervoxel_id          pt_root_id  \\\n",
       "0  730537     t   32.307937                  0                   0   \n",
       "1  373879     t  229.045043  96218056992431305  864691135538371698   \n",
       "2  601340     t  426.138010                  0                   0   \n",
       "3  201858     t   93.753836  84955554103121097  864691135373893678   \n",
       "4  600774     t  135.189791                  0                   0   \n",
       "\n",
       "               pt_position bb_start_position  bb_end_position  \n",
       "0  [381312, 273984, 19993]   [nan, nan, nan]  [nan, nan, nan]  \n",
       "1  [228816, 239776, 19593]   [nan, nan, nan]  [nan, nan, nan]  \n",
       "2  [340000, 279152, 20946]   [nan, nan, nan]  [nan, nan, nan]  \n",
       "3  [146848, 213600, 26267]   [nan, nan, nan]  [nan, nan, nan]  \n",
       "4  [339120, 276112, 19442]   [nan, nan, nan]  [nan, nan, nan]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb4dd47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110505\n"
     ]
    }
   ],
   "source": [
    "all_pt_rood_id_ser = pd.Series(prf_df.loc[:,\"pt_root_id\"])\n",
    "valid_pt_rood_ids_set = set()\n",
    "for i in range(len(all_pt_rood_id_ser)):\n",
    "    if all_pt_rood_id_ser[i] != 0:\n",
    "        valid_pt_rood_ids_set.add(all_pt_rood_id_ser[i])\n",
    "    \n",
    "print(len(valid_pt_rood_ids_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11213ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pt_rood_ids_lst = list(valid_pt_rood_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d017408",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_type_df_coverall_notallright=client.materialize.query_table('allen_soma_ei_class_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b983854a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69957"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neuron_type_df_coverall_notallright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d3e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_of_coverall_notallright_ser = pd.Series(neuron_type_df_coverall_notallright.loc[:,\"pt_root_id\"])\n",
    "types_of_coverall_notallright_ser = pd.Series(neuron_type_df_coverall_notallright.loc[:,\"cell_type\"])\n",
    "ids_of_coverall_notallright_lst = list(ids_of_coverall_notallright_ser)\n",
    "types_of_coverall_notallright_lst = list(types_of_coverall_notallright_ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed505246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # run one time and then save the result \n",
    "# # valid_ids\n",
    "# ext_neuron_valid_ids_lst = []\n",
    "# with tqdm(total= len(valid_pt_rood_ids_lst)) as pbar:\n",
    "#     for elem in valid_pt_rood_ids_lst:\n",
    "#         pbar.update(1)\n",
    "#         if elem in ids_of_coverall_notallright_lst:\n",
    "#             elem_idx = ids_of_coverall_notallright_lst.index(elem)\n",
    "#             if types_of_coverall_notallright_ser[elem_idx] == 'excitatory':\n",
    "#                 ext_neuron_valid_ids_lst.append(elem)\n",
    "\n",
    "# utils.save_obj_with_name(ext_neuron_valid_ids_lst, 'unproof_ext_neuron_valid_ids_lst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e75367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56565\n"
     ]
    }
   ],
   "source": [
    "valid_ids = utils.load_obj_from_filename('unproof_ext_neuron_valid_ids_lst')\n",
    "print(len(valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba77afd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# setting up str_num_2_presypneuron and presypneuron_2_str_num to simplify the representation of pre-syp-neuron-id\n",
    "# using string number (e.g.: \"20\") to correspond to the long-digit id of the pre-syp-neuron \n",
    "num_rep_2_presypneuron = {}\n",
    "presypneuron_2_num_rep = {}\n",
    "for i, the_id in enumerate(valid_ids):\n",
    "    num_rep_2_presypneuron[i] = the_id\n",
    "    presypneuron_2_num_rep[the_id] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98cc4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#500 Server Error reported but should try again\n",
    "# neuron_type_df_notcoverall_allright=client.materialize.query_table('allen_visp_column_soma_coarse_types_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036c187",
   "metadata": {},
   "source": [
    "# working on sheezneat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127b0be",
   "metadata": {},
   "source": [
    "2.1 After getting all the \"extended neurons\" IDs (in this case it will be the valid_ids variable that we have declared earlier on) we then go back to the method of using these IDs to find the synaptic IDs for each extended neuron ID and set up the dictionary. The dictionary will have the folloiwng structure: Key: synaptic ID (id) Vals: a tuple (pre-syn-neuron id aka pre_pt_root_id, post-syn-neuron id aka post_pt_root_id)\n",
    "\n",
    "2.2 In addition, we also transofrm the voxel coordinates into the xyz coordinates by element-wise-multiplying voxel pos and (4, 4, 40) to get um in x-y-z coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ad89516",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████▊                           | 12571/56565 [7:13:11<25:15:59,  2.07s/it]\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "502 Server Error: Bad Gateway for url: https://minnie.microns-daf.com/materialize/api/v2/datastack/minnie65_public_v117/version/117/table/synapses_pni_2/query?return_pyarrow=True&split_positions=True content:b'<html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x7/58sbjjhs0n1_gvnjck8m1ql00000gn/T/ipykernel_93160/2967265364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#create syp info and save them (only do this once, then are commented out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msyp_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyp_voxel_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyp_pos_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreating_syp_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_syp_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyp_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyp_voxel_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyp_pos_tracking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/2022 summer research/micron_loc_repo/working_dir/utils.py\u001b[0m in \u001b[0;36mcreating_syp_information\u001b[0;34m(valid_ids, verified_ids_len, client)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# outputs of the selected neuron (the selected neurons will be the pre-syp neurons)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterialize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynapse_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mpost__pt_root_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_pt_root_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mser_post__pt_root_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost__pt_root_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#post neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ofs/lib/python3.7/site-packages/caveclient/materializationengine.py\u001b[0m in \u001b[0;36msynapse_query\u001b[0;34m(self, pre_ids, post_ids, bounding_box, bounding_box_column, timestamp, remove_autapses, include_zeros, limit, offset, split_positions, synapse_table, datastack_name, materialization_version, metadata)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mdatastack_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatastack_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m             \u001b[0mmerge_reference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         )\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ofs/lib/python3.7/site-packages/caveclient/materializationengine.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(self, table, filter_in_dict, filter_out_dict, filter_equal_dict, filter_spatial_dict, select_columns, offset, limit, datastack_name, return_df, split_positions, materialization_version, timestamp, metadata, merge_reference, desired_resolution)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mreturn_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         )\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_df\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ofs/lib/python3.7/site-packages/caveclient/base.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;34m\"\"\"Raises :class:`HTTPError`, if one occurred.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ofs/lib/python3.7/site-packages/caveclient/base.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mwarning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 502 Server Error: Bad Gateway for url: https://minnie.microns-daf.com/materialize/api/v2/datastack/minnie65_public_v117/version/117/table/synapses_pni_2/query?return_pyarrow=True&split_positions=True content:b'<html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n'"
     ]
    }
   ],
   "source": [
    "#create syp info and save them (only do this once, then are commented out)\n",
    "syp_dict, syp_voxel_pos, syp_pos_trackingI’m  = utils.creating_syp_information(valid_ids, len(valid_ids), client)\n",
    "utils.save_syp_information(syp_dict, syp_voxel_pos, syp_pos_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading premade dataset  \n",
    "syp_dict_file_name = 'syp_dict'\n",
    "syp_voxel_pos_file_name = 'syp_voxel_pos'\n",
    "syp_xyz_pos_file_name = 'syp_xyz_pos'\n",
    "syp_pos_tracking_file_name = 'syp_pos_tracking'\n",
    "syp_dict, syp_voxel_pos, syp_xyz_pos, syp_pos_tracking = utils.read_in_syp_information(syp_dict_file_name, syp_voxel_pos_file_name, syp_xyz_pos_file_name, syp_pos_tracking_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syp_voxel_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syp_xyz_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195457d5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# side track to look at the data structure in syp_dict\n",
    "list_of_syp_dict_keys = list(syp_dict.keys())\n",
    "syp_dict_num_elems_per_key_dict = {}\n",
    "for key in list_of_syp_dict_keys:\n",
    "    elements_of_key = syp_dict[key]\n",
    "    number_of_elems_of_key = len(elements_of_key)\n",
    "    if number_of_elems_of_key in syp_dict_num_elems_per_key_dict:\n",
    "        syp_dict_num_elems_per_key_dict[number_of_elems_of_key] += 1\n",
    "    else:\n",
    "        syp_dict_num_elems_per_key_dict[number_of_elems_of_key] = 0\n",
    "        syp_dict_num_elems_per_key_dict[number_of_elems_of_key] += 1\n",
    "print(syp_dict_num_elems_per_key_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef8e4a",
   "metadata": {},
   "source": [
    "3. feed the coordinates info into the kd-tree using fcn from scipy. (source: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "syp_pos_kd_tree = sci_spatial.KDTree(syp_xyz_pos, 3)\n",
    "num_rows_of_syp_positions, num_cols_of_syp_positions = np.shape(syp_xyz_pos)\n",
    "radius = 1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86883de3",
   "metadata": {},
   "source": [
    "4. For each synapse, query the kd-tree with a 10 um (aka 10,000 nm) ball centered on that synapse, and for any neighboring synapse it finds, check to see if they share the same post segmentation ID. (the end goal is to have a set of tuples for which each tuple represent a candidate sequence of pre-syp-neurons (that are verified) that may be on the same dendrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ac281",
   "metadata": {},
   "source": [
    "5. constructing a dict for which key is the spatially ordered tuple and the value of the key is the number of occurance. \n",
    "\n",
    "To get the spatially ordered tuple:\n",
    "\n",
    "First, we construct a distance matrix (symmatic) for the elements within the tuples. Example: for a set of points say \"A, B, C, D\". The row will be ABCD and the column will be ABCD. Each entry of the matrix is the distance between a pair of points. (how to create a distance matrix in python: https://stackoverflow.com/questions/29481485/creating-a-distance-matrix)\n",
    "\n",
    "Second, find the largest value in the matrix and select one of the two points associate with that largest value to be the relative origin. \n",
    "\n",
    "Third, we then go to the row associated with the selected point to get all the values. And then we can figure out the spatial ordering of all the rest of the points based on the values. A smaller value corresponds to a point closer to the selected origin and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_of_syp_positions, num_cols_of_syp_positions = np.shape(syp_xyz_pos)\n",
    "print(num_rows_of_syp_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syp_pos_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ee75a",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #this part of the code takes care of the potential of multiple counting same sequence of pre-syp-neurons ]\n",
    "# #that have the same post-ysp-neurons connections and address the issue of end points of cand seq swap. \n",
    "# syp_pos_kd_tree = sci_spatial.KDTree(syp_xyz_pos, 3)\n",
    "# num_rows_of_syp_positions, num_cols_of_syp_positions = np.shape(syp_xyz_pos)\n",
    "# radius = 1e4\n",
    "\n",
    "# seqs_with_post_neuron_lst = {}\n",
    "# syp_ids_of_seqs_with_post_neuron_lst = {}\n",
    "\n",
    "# with tqdm(total= num_rows_of_syp_positions) as pbar:\n",
    "#     for i in range(num_rows_of_syp_positions):\n",
    "#         pbar.update(1)\n",
    "#         da_pt_and_its_neighbors_pos_lst = []\n",
    "#         da_pt_pos = syp_xyz_pos[i]\n",
    "#         resulting_neighbors_idxes = syp_pos_kd_tree.query_ball_point(da_pt_pos, radius, p=2.0, eps=0, workers=3, return_sorted=None, return_length=False)\n",
    "#         da_pt_syp_ID = syp_pos_tracking[i]\n",
    "#         neighbors_IDs_list = []\n",
    "#         for neighbor_pos_idx in resulting_neighbors_idxes:\n",
    "#             neighbor_ID = syp_pos_tracking[neighbor_pos_idx]\n",
    "#             neighbors_IDs_list.append(neighbor_ID)\n",
    "#         syps_with_same_post_syp_ID_lst = []\n",
    "#         neur_with_same_post_syp_ID_lst = []\n",
    "#         da_pt_post_syp_id = syp_dict[da_pt_syp_ID][0][1]\n",
    "#         for neighbor_id in neighbors_IDs_list:\n",
    "#             neighbor_pre_syp_id, neighbor_post_syp_id = syp_dict[neighbor_id][0]\n",
    "#             if neighbor_post_syp_id == da_pt_post_syp_id:\n",
    "#                 syps_with_same_post_syp_ID_lst.append(neighbor_id)\n",
    "#                 neur_with_same_post_syp_ID_lst.append(neighbor_pre_syp_id)\n",
    "#                 da_pt_and_its_neighbors_pos_lst.append(syp_xyz_pos[syp_pos_tracking.index(neighbor_id)])\n",
    "#         ### cast syps_with_post_syp_ID_lst to np array \n",
    "#         syps_with_same_post_syp_ID_lst = np.array(syps_with_same_post_syp_ID_lst)\n",
    "#         neur_with_same_post_syp_ID_lst = np.array(neur_with_same_post_syp_ID_lst)\n",
    "        \n",
    "#         ### sorted based on spatial ordering \n",
    "#         da_pt_dist_matrix = distance_matrix(da_pt_and_its_neighbors_pos_lst, da_pt_and_its_neighbors_pos_lst)\n",
    "#         da_pt_dist_mat_max_vals = np.amax(da_pt_dist_matrix)\n",
    "#         da_pt_maxval_loc_in_dist_mat = np.where(da_pt_dist_matrix == np.amax(da_pt_dist_matrix))\n",
    "#         da_pt_anchor_pt_idx = da_pt_maxval_loc_in_dist_mat[0][0] #problem with naming variable \n",
    "#         ordered_seq_idxs = np.argsort(da_pt_dist_matrix[da_pt_anchor_pt_idx]) #\n",
    "#         ordered_syn_ids = tuple(syps_with_same_post_syp_ID_lst[ordered_seq_idxs]) #make syps_... an array instead of list\n",
    "#         ordered_neur_ids = tuple(neur_with_same_post_syp_ID_lst[ordered_seq_idxs])\n",
    "        \n",
    "        \n",
    "        \n",
    "# #         da_pt_anchor_pt = syps_with_same_post_syp_ID_lst[da_pt_anchor_pt_idx]\n",
    "# #         da_pt_anchor_row_array = da_pt_dist_matrix[0] #### da_pt_anchor_pot is not being used here\n",
    "# #         da_pt_anchor_row_list = da_pt_anchor_row_array.tolist()\n",
    "# #         da_pt_anchor_row_list_with_index = []\n",
    "\n",
    "# #         for i, elem in enumerate(da_pt_anchor_row_list):\n",
    "# #             da_pt_anchor_row_list_with_index.append((elem, i))\n",
    "# #         sorted_da_pt_anchor_row_list_with_index = sorted(da_pt_anchor_row_list_with_index, key=lambda x: x[0])\n",
    "\n",
    "# #         da_pt_neighbor_spatial_ordered_idx = []\n",
    "# #         for elem in sorted_da_pt_anchor_row_list_with_index:\n",
    "# #             da_pt_neighbor_spatial_ordered_idx.append(elem[1])\n",
    "\n",
    "# #         da_pt_neighbor_pre_syp_neuron_ordered_IDs_lst = []\n",
    "# #         for i in da_pt_neighbor_spatial_ordered_idx:\n",
    "# #             syp_id = syps_with_same_post_syp_ID_lst[i]\n",
    "# #             pre_syp_neuron, post_syp_neuron = syp_dict[syp_id][0]\n",
    "# #             num_rep_neighbor_pre_syp_id = presypneuron_2_num_rep[pre_syp_neuron]\n",
    "# #             if not num_rep_neighbor_pre_syp_id in da_pt_neighbor_pre_syp_neuron_ordered_IDs_lst:\n",
    "# #                 da_pt_neighbor_pre_syp_neuron_ordered_IDs_lst.append(num_rep_neighbor_pre_syp_id)\n",
    "# #         da_pt_neighbor_pre_syp_neuron_ordered_IDs_lst_tup = tuple(da_pt_neighbor_pre_syp_neuron_ordered_IDs_lst)\n",
    "\n",
    "#         if not ordered_neur_ids in seqs_with_post_neuron_lst:\n",
    "#             seqs_with_post_neuron_lst[ordered_neur_ids] = [da_pt_post_syp_id]\n",
    "#             syp_ids_of_seqs_with_post_neuron_lst[ordered_neur_ids] = [ordered_syn_ids]\n",
    "#         else:\n",
    "#             if not da_pt_post_syp_id in seqs_with_post_neuron_lst[ordered_neur_ids]:\n",
    "#                 seqs_with_post_neuron_lst[ordered_neur_ids].append(da_pt_post_syp_id)\n",
    "#                 syp_ids_of_seqs_with_post_neuron_lst[ordered_neur_ids].append(ordered_syn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a0765",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# #only do it once then commented out \n",
    "# #for 5um radius save the seqs_with_post_neuron_lst and syp_ids_of_seqs_with_post_neuron_lst\n",
    "# seqs_with_post_neuron_lst_5um = seqs_with_post_neuron_lst\n",
    "# syp_ids_of_seqs_with_post_neuron_lst_5um = syp_ids_of_seqs_with_post_neuron_lst \n",
    "# utils.save_obj_with_name(seqs_with_post_neuron_lst_5um, 'seqs_with_post_neuron_lst_5um')\n",
    "# utils.save_obj_with_name(syp_ids_of_seqs_with_post_neuron_lst_5um, 'syp_ids_of_seqs_with_post_neuron_lst_5um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e3ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only do it once then commented out \n",
    "# #for 10um radius save the seqs_with_post_neuron_lst and syp_ids_of_seqs_with_post_neuron_lst\n",
    "# seqs_with_post_neuron_lst_10um = seqs_with_post_neuron_lst\n",
    "# syp_ids_of_seqs_with_post_neuron_lst_10um = syp_ids_of_seqs_with_post_neuron_lst \n",
    "# utils.save_obj_with_name(seqs_with_post_neuron_lst_10um, 'seqs_with_post_neuron_lst_10um')\n",
    "# utils.save_obj_with_name(syp_ids_of_seqs_with_post_neuron_lst_10um, 'syp_ids_of_seqs_with_post_neuron_lst_10um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61648b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved results \n",
    "seqs_with_post_neuron_lst = utils.load_obj_from_filename('seqs_with_post_neuron_lst_10um')\n",
    "syp_ids_of_seqs_with_post_neuron_lst = utils.load_obj_from_filename('syp_ids_of_seqs_with_post_neuron_lst_10um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed3a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_keys = filter(lambda key: len(key)==4, seqs_with_post_neuron_lst)\n",
    "for key in seq_keys:\n",
    "    print([k%10000 for k in key], len(seqs_with_post_neuron_lst[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences = [len(seqs_with_post_neuron_lst[key]) for key in seqs_with_post_neuron_lst if len(key)==3]\n",
    "occ_array = np.array(occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cfa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('using radius = 5um for the kd tree')\n",
    "legends = []\n",
    "for i in range(3, 7):\n",
    "    occurences = [len(seqs_with_post_neuron_lst[key]) for key in seqs_with_post_neuron_lst if len(key)==i]\n",
    "    occ_array = np.array(occurences)\n",
    "    print('ttl number of seqs with len' + str(i) + ' = ' + str(len(occ_array)))\n",
    "    plt.plot(np.sort(occ_array), np.linspace(0, 1, len(occ_array), endpoint=False))\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.xscale(\"log\")\n",
    "    legends.append('len' + str(i))\n",
    "plt.legend(legends)\n",
    "plt.title(\"cdf of number of occurance on log scale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42a3be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(occ_array))\n",
    "plt.plot(np.sort(occ_array), np.linspace(0, 1, len(occ_array), endpoint=False))\n",
    "plt.ylim([0.98, 1])\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d9518d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occurences4 = [len(seqs_with_post_neuron_lst[key]) for key in seqs_with_post_neuron_lst if len(key)==4]\n",
    "occ_array4 = np.array(occurences4)\n",
    "print(len(occ_array4))\n",
    "plt.plot(np.sort(occ_array4), np.linspace(0, 1, len(occ_array4), endpoint=False))\n",
    "plt.ylim([0.98, 1])\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8627f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occurences5 = [len(seqs_with_post_neuron_lst[key]) for key in seqs_with_post_neuron_lst if len(key)==5]\n",
    "occ_array5 = np.array(occurences5)\n",
    "print(len(occ_array5))\n",
    "plt.plot(np.sort(occ_array5), np.linspace(0, 1, len(occ_array5), endpoint=False))\n",
    "plt.ylim([0.98, 1])\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38022a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lst = utils.get_seq_with_length_n_of_unique_keys_for_occur_more_than_m_times_w_type_lst(3, 0, seqs_with_post_neuron_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92846f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(a_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utils.get_seq_with_length_n_of_unique_keys_for_occur_more_than_m_times_w_type_lst(4, 0, seqs_with_post_neuron_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utils.get_seq_with_length_n_of_unique_keys_for_occur_more_than_m_times_w_type_lst(5, 0, seqs_with_post_neuron_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_3_seq_morethan_1_time_w_post_neuron_lst = utils.get_seq_with_length_n_of_unique_keys_for_occur_more_than_m_times_w_type_lst(5, 0, seqs_with_post_neuron_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_3_seq_morethan_1_time_w_syp_ids_lst = utils.get_seq_with_length_n_of_unique_keys_for_occur_more_than_m_times_w_type_lst(3, 0, syp_ids_of_seqs_with_post_neuron_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_collections = utils.neuroglancer_render(client, statebuilder, len_3_seq_morethan_1_time_w_post_neuron_lst, len_3_seq_morethan_1_time_w_syp_ids_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_collections_pd = pd.DataFrame([{'neurons': key, 'url':value} for key, value in url_collections.items()])\n",
    "# url_collections_pd.to_csv('len_3_seq_all_uniqe_url_collections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbb5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.round(np.logspace(np.log10(60), np.log10(480), 8, base=10)).astype(int)\n",
    "cts = np.zeros((3,8,20))\n",
    "for k,p in enumerate([3, 4, 5]):\n",
    "    a_lst = utils.get_seq_with_length_n_of_unique_keys_for_occur_more_than_m_times_w_type_lst(p, 0, seqs_with_post_neuron_lst)\n",
    "    for j, n in enumerate(ns):\n",
    "        for i in range(20):\n",
    "            sampled_neurons = random.sample(valid_ids, n)\n",
    "            ct = 0 \n",
    "            for elem in a_lst:\n",
    "                ct += np.all([e in sampled_neurons for e in elem])\n",
    "            cts[k,j,i] = ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa842eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts[0].mean(axis=1)\n",
    "# cts[1].mean(axis=1)\n",
    "# cts[2].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
    "for k,p in enumerate([3, 4, 5]):\n",
    "    means = cts[k].mean(axis=1)\n",
    "    stds = cts[k].std(axis=1)\n",
    "    means_lower = np.clip(means - stds, 0, None)\n",
    "    means_upper = means + stds\n",
    "    plt.scatter(ns, means, color = colors[k])\n",
    "    plt.fill_between(ns, means_lower, means_upper, alpha = 0.1)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff10444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
